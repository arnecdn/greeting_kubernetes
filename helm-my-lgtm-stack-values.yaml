grafana:
  admin:
    existingSecret: ""
    passwordKey: admin-password
    userKey: admin-user
  adminUser: admin
  affinity: {}
  alerting: {}
  assertNoLeakedSecrets: true
  automountServiceAccountToken: true
  autoscaling:
    behavior: {}
    enabled: false
    maxReplicas: 5
    minReplicas: 1
    targetCPU: "60"
    targetMemory: ""
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault
  createConfigmap: true
  dashboardProviders: {}
  dashboards: {}
  dashboardsConfigMaps: {}
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - isDefault: false
        name: Loki
        type: loki
        uid: loki
        url: http://{{ .Release.Name }}-loki-gateway
      - isDefault: true
        name: Mimir
        type: prometheus
        uid: prom
        url: http://{{ .Release.Name }}-mimir-nginx/prometheus
      - isDefault: false
        jsonData:
          lokiSearch:
            datasourceUid: loki
          serviceMap:
            datasourceUid: prom
          tracesToLogsV2:
            datasourceUid: loki
          tracesToMetrics:
            datasourceUid: prom
        name: Tempo
        type: tempo
        uid: tempo
        url: http://{{ .Release.Name }}-tempo-query-frontend:3100
  deploymentStrategy:
    type: RollingUpdate
  dnsConfig: {}
  dnsPolicy: null
  downloadDashboards:
    env: {}
    envFromSecret: ""
    envValueFrom: {}
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
  downloadDashboardsImage:
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: curlimages/curl
    sha: ""
    tag: 7.85.0
  enableKubeBackwardCompatibility: false
  enableServiceLinks: true
  enabled: true
  env: {}
  envFromConfigMaps: []
  envFromSecret: ""
  envFromSecrets: []
  envRenderSecret: {}
  envValueFrom: {}
  extraConfigmapMounts: []
  extraContainerVolumes: []
  extraContainers: ""
  extraEmptyDirMounts: []
  extraExposePorts: []
  extraInitContainers: []
  extraLabels: {}
  extraObjects: []
  extraSecretMounts: []
  extraVolumeMounts: []
  extraVolumes: []
  global:
    imagePullSecrets: []
    imageRegistry: null
  gossipPortName: gossip
  grafana.ini:
    analytics:
      check_for_updates: true
    grafana_net:
      url: https://grafana.net
    log:
      mode: console
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    server:
      domain: '{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ .Values.ingress.hosts
        | first }}{{ else }}''''{{ end }}'
  headlessService: false
  hostAliases: []
  image:
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/grafana
    sha: ""
    tag: ""
  imageRenderer:
    affinity: {}
    autoscaling:
      behavior: {}
      enabled: false
      maxReplicas: 5
      minReplicas: 1
      targetCPU: "60"
      targetMemory: ""
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      seccompProfile:
        type: RuntimeDefault
    deploymentStrategy: {}
    enabled: false
    env:
      HTTP_HOST: 0.0.0.0
    envValueFrom: {}
    grafanaProtocol: http
    grafanaSubPath: ""
    hostAliases: []
    image:
      pullPolicy: Always
      registry: docker.io
      repository: grafana/grafana-image-renderer
      sha: ""
      tag: latest
    networkPolicy:
      extraIngressSelectors: []
      limitEgress: false
      limitIngress: true
    nodeSelector: {}
    podAnnotations: {}
    podPortName: http
    priorityClassName: ""
    replicas: 1
    resources: {}
    revisionHistoryLimit: 10
    securityContext: {}
    service:
      appProtocol: ""
      enabled: true
      port: 8081
      portName: http
      targetPort: 8081
    serviceAccountName: ""
    serviceMonitor:
      enabled: false
      interval: 1m
      labels: {}
      path: /metrics
      relabelings: []
      scheme: http
      scrapeTimeout: 30s
      targetLabels: []
      tlsConfig: {}
    tolerations: []
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  initChownData:
    enabled: true
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: library/busybox
      sha: ""
      tag: 1.31.1
    resources: {}
    securityContext:
      capabilities:
        add:
        - CHOWN
      runAsNonRoot: false
      runAsUser: 0
      seccompProfile:
        type: RuntimeDefault
  ldap:
    config: ""
    enabled: false
    existingSecret: ""
  lifecycleHooks: {}
  livenessProbe:
    failureThreshold: 10
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
  namespaceOverride: ""
  networkPolicy:
    allowExternal: true
    egress:
      blockDNSResolution: false
      enabled: false
      ports: []
      to: []
    enabled: false
    explicitNamespacesSelector: {}
    ingress: true
  nodeSelector: {}
  notifiers: {}
  persistence:
    accessModes:
    - ReadWriteOnce
    enabled: false
    extraPvcLabels: {}
    finalizers:
    - kubernetes.io/pvc-protection
    inMemory:
      enabled: false
    size: 10Gi
    type: pvc
  plugins: []
  podDisruptionBudget: {}
  podPortName: grafana
  rbac:
    create: true
    extraClusterRoleRules: []
    extraRoleRules: []
    namespaced: false
    pspEnabled: false
    pspUseAppArmor: false
  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000
  replicas: 1
  resources: {}
  revisionHistoryLimit: 10
  securityContext:
    fsGroup: 472
    runAsGroup: 472
    runAsNonRoot: true
    runAsUser: 472
  service:
    annotations: {}
    appProtocol: ""
    enabled: true
    labels: {}
    loadBalancerClass: ""
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    port: 80
    portName: service
    targetPort: 3000
    type: ClusterIP
  serviceAccount:
    automountServiceAccountToken: false
    create: true
    labels: {}
    name: null
    nameTest: null
  serviceMonitor:
    enabled: false
    interval: 30s
    labels: {}
    metricRelabelings: []
    path: /metrics
    relabelings: []
    scheme: http
    scrapeTimeout: 30s
    targetLabels: []
    tlsConfig: {}
  sidecar:
    alerts:
      enabled: false
      env: {}
      extraMounts: []
      initAlerts: false
      label: grafana_alert
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/alerting/reload
      resource: both
      script: null
      searchNamespace: null
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    dashboards:
      SCProvider: true
      defaultFolderName: null
      enabled: false
      env: {}
      extraMounts: []
      folder: /tmp/dashboards
      folderAnnotation: null
      label: grafana_dashboard
      labelValue: ""
      provider:
        allowUiUpdates: false
        disableDelete: false
        folder: ""
        foldersFromFilesStructure: false
        name: sidecarProvider
        orgid: 1
        type: file
      reloadURL: http://localhost:3000/api/admin/provisioning/dashboards/reload
      resource: both
      script: null
      searchNamespace: null
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    datasources:
      enabled: false
      env: {}
      envValueFrom: {}
      initDatasources: false
      label: grafana_datasource
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/datasources/reload
      resource: both
      script: null
      searchNamespace: null
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    enableUniqueFilenames: false
    image:
      registry: quay.io
      repository: kiwigrid/k8s-sidecar
      sha: ""
      tag: 1.26.1
    imagePullPolicy: IfNotPresent
    livenessProbe: {}
    notifiers:
      enabled: false
      env: {}
      initNotifiers: false
      label: grafana_notifier
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/notifications/reload
      resource: both
      script: null
      searchNamespace: null
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    plugins:
      enabled: false
      env: {}
      initPlugins: false
      label: grafana_plugin
      labelValue: ""
      reloadURL: http://localhost:3000/api/admin/provisioning/plugins/reload
      resource: both
      script: null
      searchNamespace: null
      sizeLimit: {}
      skipReload: false
      watchMethod: WATCH
    readinessProbe: {}
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
  smtp:
    existingSecret: ""
    passwordKey: password
    userKey: user
  testFramework:
    enabled: true
    image:
      registry: docker.io
      repository: bats/bats
      tag: v1.4.1
    imagePullPolicy: IfNotPresent
    resources: {}
    securityContext: {}
  tolerations: []
  topologySpreadConstraints: []
  useStatefulSet: false
grafana-oncall:
  enabled: false
loki:
  compactor:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.compactorSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.compactorSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    command: null
    enabled: false
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    initContainers: []
    kind: StatefulSet
    livenessProbe: {}
    nodeSelector: {}
    persistence:
      annotations: {}
      claims:
      - name: data
        size: 10Gi
        storageClass: null
      enableStatefulSetAutoDeletePVC: false
      enabled: false
      size: 10Gi
      storageClass: null
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    readinessProbe: {}
    replicas: 1
    resources: {}
    serviceAccount:
      annotations: {}
      automountServiceAccountToken: true
      create: false
      imagePullSecrets: []
      name: null
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  distributor:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.distributorSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.distributorSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    autoscaling:
      behavior:
        enabled: false
        scaleDown: {}
        scaleUp: {}
      customMetrics: []
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    command: null
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    maxSurge: 0
    maxUnavailable: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  enabled: true
  fullnameOverride: null
  gateway:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.gatewaySelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.gatewaySelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    autoscaling:
      behavior:
        enabled: false
        scaleDown: {}
        scaleUp: {}
      customMetrics: []
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    basicAuth:
      enabled: false
      existingSecret: null
      htpasswd: '{{ htpasswd (required "''gateway.basicAuth.username'' is required"
        .Values.gateway.basicAuth.username) (required "''gateway.basicAuth.password''
        is required" .Values.gateway.basicAuth.password) }}'
      password: null
      username: null
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    deploymentStrategy:
      type: RollingUpdate
    dnsConfig: {}
    enabled: true
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: nginxinc/nginx-unprivileged
      tag: 1.20.2-alpine
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - host: gateway.loki.example.com
        paths:
        - path: /
      ingressClassName: ""
      tls: []
    livenessProbe:
      httpGet:
        path: /
        port: http
      initialDelaySeconds: 30
    maxUnavailable: null
    nginxConfig:
      file: |
        worker_processes  5;  ## Default: 1
        error_log  /dev/stderr;
        pid        /tmp/nginx.pid;
        worker_rlimit_nofile 8192;

        events {
          worker_connections  4096;  ## Default: 1024
        }

        http {
          client_body_temp_path /tmp/client_temp;
          proxy_temp_path       /tmp/proxy_temp_path;
          fastcgi_temp_path     /tmp/fastcgi_temp;
          uwsgi_temp_path       /tmp/uwsgi_temp;
          scgi_temp_path        /tmp/scgi_temp;

          proxy_http_version    1.1;

          default_type application/octet-stream;
          log_format   {{ .Values.gateway.nginxConfig.logFormat }}

          {{- if .Values.gateway.verboseLogging }}
          access_log   /dev/stderr  main;
          {{- else }}

          map $status $loggable {
            ~^[23]  0;
            default 1;
          }
          access_log   /dev/stderr  main  if=$loggable;
          {{- end }}

          sendfile     on;
          tcp_nopush   on;
          {{- if .Values.gateway.nginxConfig.resolver }}
          resolver {{ .Values.gateway.nginxConfig.resolver }};
          {{- else }}
          resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
          {{- end }}

          {{- with .Values.gateway.nginxConfig.httpSnippet }}
          {{ . | nindent 2 }}
          {{- end }}

          server {
            listen             8080;

            {{- if .Values.gateway.basicAuth.enabled }}
            auth_basic           "Loki";
            auth_basic_user_file /etc/nginx/secrets/.htpasswd;
            {{- end }}

            location = / {
              return 200 'OK';
              auth_basic off;
              access_log off;
            }

            location = /api/prom/push {
              set $api_prom_push_backend http://{{ include "loki.distributorFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       $api_prom_push_backend:3100$request_uri;
              proxy_http_version 1.1;
            }

            location = /api/prom/tail {
              set $api_prom_tail_backend http://{{ include "loki.querierFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       $api_prom_tail_backend:3100$request_uri;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection "upgrade";
              proxy_http_version 1.1;
            }

            # Ruler
            location ~ /prometheus/api/v1/alerts.* {
              proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }
            location ~ /prometheus/api/v1/rules.* {
              proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }
            location ~ /api/prom/rules.* {
              proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }
            location ~ /api/prom/alerts.* {
              proxy_pass       http://{{ include "loki.rulerFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            location ~ /api/prom/.* {
              set $api_prom_backend http://{{ include "loki.queryFrontendFullname" . }}-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       $api_prom_backend:3100$request_uri;
              proxy_http_version 1.1;
            }

            location = /loki/api/v1/push {
              set $loki_api_v1_push_backend http://{{ include "loki.distributorFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       $loki_api_v1_push_backend:3100$request_uri;
              proxy_http_version 1.1;
            }

            location = /loki/api/v1/tail {
              set $loki_api_v1_tail_backend http://{{ include "loki.querierFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       $loki_api_v1_tail_backend:3100$request_uri;
              proxy_set_header Upgrade $http_upgrade;
              proxy_set_header Connection "upgrade";
              proxy_http_version 1.1;
            }

            location ~ /loki/api/.* {
              set $loki_api_backend http://{{ include "loki.queryFrontendFullname" . }}-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       $loki_api_backend:3100$request_uri;
              proxy_http_version 1.1;
            }

            {{- with .Values.gateway.nginxConfig.serverSnippet }}
            {{ . | nindent 4 }}
            {{- end }}
          }
        }
      httpSnippet: ""
      logFormat: |-
        main '$remote_addr - $remote_user [$time_local]  $status '
                '"$request" $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';
      resolver: ""
      serverSnippet: ""
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      fsGroup: 101
      runAsGroup: 101
      runAsNonRoot: true
      runAsUser: 101
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /
        port: http
      initialDelaySeconds: 15
      timeoutSeconds: 1
    replicas: 1
    resources: {}
    service:
      annotations: {}
      appProtocol: null
      clusterIP: null
      labels: {}
      loadBalancerIP: null
      loadBalancerSourceRanges: []
      nodePort: null
      port: 80
      type: ClusterIP
    terminationGracePeriodSeconds: 30
    tolerations: []
    verboseLogging: true
  global:
    clusterDomain: cluster.local
    dnsNamespace: kube-system
    dnsService: kube-dns
    image:
      registry: null
    priorityClassName: null
  hostAliases: []
  imagePullSecrets: []
  indexGateway:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.indexGatewaySelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.indexGatewaySelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    enabled: false
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    initContainers: []
    joinMemberlist: true
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      annotations: {}
      enableStatefulSetAutoDeletePVC: false
      enabled: false
      inMemory: false
      size: 10Gi
      storageClass: null
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 300
    tolerations: []
  ingester:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.ingesterSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.ingesterSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    autoscaling:
      behavior:
        enabled: false
        scaleDown: {}
        scaleUp: {}
      customMetrics: []
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    command: null
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    initContainers: []
    kind: StatefulSet
    lifecycle: {}
    livenessProbe: {}
    maxSurge: 0
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      claims:
      - name: data
        size: 10Gi
        storageClass: null
      enableStatefulSetAutoDeletePVC: false
      enabled: false
      inMemory: false
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    readinessProbe: {}
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 300
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "loki.ingesterSelectorLabels" . | nindent 6 }}
  ingress:
    annotations: {}
    enabled: false
    hosts:
    - loki.example.com
    paths:
      distributor:
      - /api/prom/push
      - /loki/api/v1/push
      querier:
      - /api/prom/tail
      - /loki/api/v1/tail
      query-frontend:
      - /loki/api
      ruler:
      - /api/prom/rules
      - /loki/api/v1/rules
      - /prometheus/api/v1/rules
      - /prometheus/api/v1/alerts
  loki:
    annotations: {}
    appProtocol: ""
    command: null
    config: |
      auth_enabled: false

      server:
        {{- toYaml .Values.loki.server | nindent 6 }}

      common:
        compactor_address: http://{{ include "loki.compactorFullname" . }}:3100

      distributor:
        ring:
          kvstore:
            store: memberlist

      memberlist:
        join_members:
          - {{ include "loki.fullname" . }}-memberlist

      ingester_client:
        grpc_client_config:
          grpc_compression: gzip

      ingester:
        lifecycler:
          ring:
            kvstore:
              store: memberlist
            replication_factor: 1
        chunk_idle_period: 30m
        chunk_block_size: 262144
        chunk_encoding: snappy
        chunk_retain_period: 1m
        max_transfer_retries: 0
        wal:
          dir: /var/loki/wal

      limits_config:
        enforce_metric_name: false
        reject_old_samples: true
        reject_old_samples_max_age: 168h
        max_cache_freshness_per_query: 10m
        split_queries_by_interval: 15m

      {{- if .Values.loki.schemaConfig}}
      schema_config:
      {{- toYaml .Values.loki.schemaConfig | nindent 2}}
      {{- end}}
      {{- if .Values.loki.storageConfig}}
      storage_config:
      {{- if .Values.indexGateway.enabled}}
      {{- $indexGatewayClient := dict "server_address" (printf "dns:///%s:9095" (include "loki.indexGatewayFullname" .)) }}
      {{- $_ := set .Values.loki.storageConfig.boltdb_shipper "index_gateway_client" $indexGatewayClient }}
      {{- end}}
      {{- toYaml .Values.loki.storageConfig | nindent 2}}
      {{- if .Values.memcachedIndexQueries.enabled }}
        index_queries_cache_config:
          memcached_client:
            addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedIndexQueriesFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
            consistent_hash: true
      {{- end}}
      {{- end}}

      runtime_config:
        file: /var/{{ include "loki.name" . }}-runtime/runtime.yaml

      chunk_store_config:
        max_look_back_period: 0s
        {{- if .Values.memcachedChunks.enabled }}
        chunk_cache_config:
          embedded_cache:
            enabled: false
          memcached_client:
            consistent_hash: true
            addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedChunksFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
        {{- end }}
        {{- if .Values.memcachedIndexWrites.enabled }}
        write_dedupe_cache_config:
          memcached_client:
            consistent_hash: true
            addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedIndexWritesFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
        {{- end }}

      table_manager:
        retention_deletes_enabled: false
        retention_period: 0s

      query_range:
        align_queries_with_step: true
        max_retries: 5
        cache_results: true
        results_cache:
          cache:
            {{- if .Values.memcachedFrontend.enabled }}
            memcached_client:
              addresses: dnssrv+_memcached-client._tcp.{{ include "loki.memcachedFrontendFullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}
              consistent_hash: true
            {{- else }}
            embedded_cache:
              enabled: true
              ttl: 24h
            {{- end }}

      frontend_worker:
        {{- if .Values.queryScheduler.enabled }}
        scheduler_address: {{ include "loki.querySchedulerFullname" . }}:9095
        {{- else }}
        frontend_address: {{ include "loki.queryFrontendFullname" . }}-headless:9095
        {{- end }}

      frontend:
        log_queries_longer_than: 5s
        compress_responses: true
        {{- if .Values.queryScheduler.enabled }}
        scheduler_address: {{ include "loki.querySchedulerFullname" . }}:9095
        {{- end }}
        tail_proxy_url: http://{{ include "loki.querierFullname" . }}:3100

      compactor:
        shared_store: filesystem
        working_directory: /var/loki/compactor

      ruler:
        storage:
          type: local
          local:
            directory: /etc/loki/rules
        ring:
          kvstore:
            store: memberlist
        rule_path: /tmp/loki/scratch
        alertmanager_url: https://alertmanager.xx
        external_url: https://alertmanager.xx
    configAsSecret: false
    configSecretAnnotations: {}
    configSecretLabels: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    existingSecretForConfig: ""
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: grafana/loki
      tag: null
    livenessProbe:
      httpGet:
        path: /ready
        port: http
      initialDelaySeconds: 300
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    readinessProbe:
      httpGet:
        path: /ready
        port: http
      initialDelaySeconds: 30
      timeoutSeconds: 1
    revisionHistoryLimit: 10
    schemaConfig:
      configs:
      - from: "2020-09-07"
        index:
          period: 24h
          prefix: loki_index_
        object_store: filesystem
        schema: v11
        store: boltdb-shipper
    server:
      http_listen_port: 3100
    serviceAnnotations: {}
    storageConfig:
      boltdb_shipper:
        active_index_directory: /var/loki/index
        cache_location: /var/loki/cache
        cache_ttl: 168h
        shared_store: filesystem
      filesystem:
        directory: /var/loki/chunks
    structuredConfig: {}
  memcached:
    appProtocol: ""
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: memcached
      tag: 1.6.21-alpine
    livenessProbe:
      initialDelaySeconds: 10
      tcpSocket:
        port: http
    podLabels: {}
    podSecurityContext:
      fsGroup: 11211
      runAsGroup: 11211
      runAsNonRoot: true
      runAsUser: 11211
    readinessProbe:
      initialDelaySeconds: 5
      tcpSocket:
        port: http
      timeoutSeconds: 1
    serviceAnnotations: {}
  memcachedChunks:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.memcachedChunksSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.memcachedChunksSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    enabled: false
    extraArgs:
    - -I 32m
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    hostAliases: []
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
    volumeClaimTemplates: []
  memcachedExporter:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabled: false
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: prom/memcached-exporter
      tag: v0.13.0
    podLabels: {}
    resources: {}
  memcachedFrontend:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.memcachedFrontendSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.memcachedFrontendSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    enabled: false
    extraArgs:
    - -I 32m
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    hostAliases: []
    maxUnavailable: 1
    nodeSelector: {}
    persistence:
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  memcachedIndexQueries:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.memcachedIndexQueriesSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.memcachedIndexQueriesSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    enabled: false
    extraArgs:
    - -I 32m
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    hostAliases: []
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  memcachedIndexWrites:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.memcachedIndexWritesSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.memcachedIndexWritesSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    enabled: false
    extraArgs:
    - -I 32m
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    hostAliases: []
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  nameOverride: null
  networkPolicy:
    alertmanager:
      namespaceSelector: {}
      podSelector: {}
      port: 9093
    discovery:
      namespaceSelector: {}
      podSelector: {}
      port: null
    enabled: false
    externalStorage:
      cidrs: []
      ports: []
    ingress:
      namespaceSelector: {}
      podSelector: {}
    metrics:
      cidrs: []
      namespaceSelector: {}
      podSelector: {}
  prometheusRule:
    annotations: {}
    enabled: false
    groups: []
    labels: {}
    namespace: null
  querier:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.querierSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.querierSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    autoscaling:
      behavior:
        enabled: false
        scaleDown: {}
        scaleUp: {}
      customMetrics: []
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    command: null
    dnsConfig: {}
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    initContainers: []
    maxSurge: 0
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      annotations: {}
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "loki.querierSelectorLabels" . | nindent 6 }}
  queryFrontend:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.queryFrontendSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.queryFrontendSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    autoscaling:
      behavior:
        enabled: false
        scaleDown: {}
        scaleUp: {}
      customMetrics: []
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    command: null
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    maxUnavailable: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  queryScheduler:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.querySchedulerSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.querySchedulerSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    enabled: false
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    maxUnavailable: 1
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 2
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  rbac:
    pspEnabled: false
    sccEnabled: false
  ruler:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.rulerSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.rulerSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    appProtocol:
      grpc: ""
    command: null
    directories: {}
    dnsConfig: {}
    enabled: false
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    initContainers: []
    kind: Deployment
    maxUnavailable: null
    nodeSelector: {}
    persistence:
      annotations: {}
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 300
    tolerations: []
  runtimeConfig: {}
  serviceAccount:
    annotations: {}
    automountServiceAccountToken: true
    create: true
    imagePullSecrets: []
    labels: {}
    name: null
  serviceMonitor:
    annotations: {}
    enabled: false
    interval: null
    labels: {}
    matchExpressions: []
    metricRelabelings: []
    namespace: null
    namespaceSelector: {}
    relabelings: []
    scheme: http
    scrapeTimeout: null
    targetLabels: []
    tlsConfig: null
  tableManager:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "loki.tableManagerSelectorLabels" . | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "loki.tableManagerSelectorLabels" . | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    command: null
    enabled: false
    extraArgs: []
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      registry: null
      repository: null
      tag: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    resources: {}
    serviceLabels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
mimir:
  admin-cache:
    affinity: {}
    allocatedMemory: 64
    annotations: {}
    connectionLimit: 16384
    enabled: false
    extraArgs: {}
    extraContainers: []
    initContainers: []
    maxItemMemory: 1
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: {}
  admin_api:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: null
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  alertmanager:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabled: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    fallbackConfig: |
      receivers:
          - name: default-receiver
      route:
          receiver: default-receiver
    initContainers: []
    jaegerReporterMaxQueueSize: null
    nodeSelector: {}
    persistence:
      subPath: null
    persistentVolume:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enableRetentionPolicy: false
      enabled: true
      size: 1Gi
      subPath: ""
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 32Mi
    schedulerName: ""
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    statefulSet:
      enabled: true
    statefulStrategy:
      type: RollingUpdate
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    zoneAwareReplication:
      enabled: false
      maxUnavailable: 2
      migration:
        enabled: false
        writePath: false
      topologyKey: null
      zones:
      - extraAffinity: {}
        name: zone-a
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-b
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-c
        nodeSelector: null
        storageClass: null
  chunks-cache:
    affinity: {}
    allocatedMemory: 8192
    annotations: {}
    connectionLimit: 16384
    enabled: false
    extraArgs: {}
    extraContainers: []
    extraExtendedOptions: ""
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    maxItemMemory: 1
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: {}
  compactor:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: null
    nodeSelector: {}
    persistentVolume:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enableRetentionPolicy: false
      enabled: true
      size: 2Gi
      subPath: ""
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: OrderedReady
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 60
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 512Mi
    schedulerName: ""
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 240
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  configStorageType: ConfigMap
  continuous_test:
    affinity: {}
    annotations: {}
    auth:
      bearerToken: null
      password: null
      tenant: mimir-continuous-test
      type: tenantId
    containerSecurityContext:
      readOnlyRootFilesystem: true
    enabled: false
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    image:
      pullPolicy: IfNotPresent
      repository: grafana/mimir-continuous-test
      tag: 2.12.0
    initContainers: []
    jaegerReporterMaxQueueSize: null
    maxQueryAge: 48h
    nodeSelector: {}
    numSeries: 1000
    priorityClassName: null
    replicas: 1
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: "1"
        memory: 512Mi
    runInterval: 5m
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    terminationGracePeriodSeconds: 30
    tolerations: []
  distributor:
    validation:
      max-native-histogram-buckets: 160
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: 1000
    kedaAutoscaling:
      behavior:
        scaleDown:
          policies:
          - periodSeconds: 600
            type: Percent
            value: 10
      enabled: false
      maxReplicaCount: 10
      minReplicaCount: 1
      preserveReplicas: false
      targetCPUUtilizationPercentage: 100
      targetMemoryUtilizationPercentage: 100
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 512Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 100
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  enabled: true
  enterprise:
    enabled: false
    image:
      repository: grafana/enterprise-metrics
      tag: v2.12.0
    legacyLabels: false
  externalConfigSecretName: '{{ include "mimir.resourceName" (dict "ctx" . "component"
    "config") }}'
  externalConfigVersion: "0"
  extraObjects: []
  fullnameOverride: null
  gateway:
    affinity: {}
    annotations: {}
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 70
      targetMemoryUtilizationPercentage: 70
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabledNonEnterprise: false
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - host: '{{ .Release.Name }}.mimir.example.com'
        paths:
        - path: /
      ingressClassName: ""
      nameOverride: ""
      tls:
      - hosts:
        - '{{ .Release.Name }}.mimir.example.com'
        secretName: mimir-tls
    initContainers: []
    jaegerReporterMaxQueueSize: null
    nginx:
      basicAuth:
        enabled: false
        existingSecret: null
        htpasswd: '{{ htpasswd (required "''gateway.nginx.basicAuth.username'' is
          required" .Values.gateway.nginx.basicAuth.username) (required "''gateway.nginx.basicAuth.password''
          is required" .Values.gateway.nginx.basicAuth.password) }}'
        password: null
        username: null
      config:
        accessLogEnabled: true
        enableIPv6: true
        errorLogLevel: error
        file: |
          worker_processes  5;  ## Default: 1
          error_log  /dev/stderr {{ .Values.gateway.nginx.config.errorLogLevel }};
          pid        /tmp/nginx.pid;
          worker_rlimit_nofile 8192;

          events {
            worker_connections  4096;  ## Default: 1024
          }

          http {
            client_body_temp_path /tmp/client_temp;
            proxy_temp_path       /tmp/proxy_temp_path;
            fastcgi_temp_path     /tmp/fastcgi_temp;
            uwsgi_temp_path       /tmp/uwsgi_temp;
            scgi_temp_path        /tmp/scgi_temp;

            default_type application/octet-stream;
            log_format   {{ .Values.gateway.nginx.config.logFormat }}

            {{- if .Values.gateway.nginx.verboseLogging }}
            access_log   /dev/stderr  main;
            {{- else }}

            map $status $loggable {
              ~^[23]  0;
              default 1;
            }
            access_log   {{ .Values.gateway.nginx.config.accessLogEnabled | ternary "/dev/stderr  main  if=$loggable;" "off;" }}
            {{- end }}

            sendfile           on;
            tcp_nopush         on;
            proxy_http_version 1.1;

            {{- if .Values.gateway.nginx.config.resolver }}
            resolver {{ .Values.gateway.nginx.config.resolver }};
            {{- else }}
            resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
            {{- end }}

            {{- with .Values.gateway.nginx.config.httpSnippet }}
            {{ . | nindent 2 }}
            {{- end }}

            # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
            map $http_x_scope_orgid $ensured_x_scope_orgid {
              default $http_x_scope_orgid;
              "" "{{ include "mimir.noAuthTenant" . }}";
            }

            map $http_x_scope_orgid $has_multiple_orgid_headers {
              default 0;
              "~^.+,.+$" 1;
            }

            proxy_read_timeout 300;
            server {
              listen {{ include "mimir.serverHttpListenPort" . }};
              {{- if .Values.gateway.nginx.config.enableIPv6 }}
              listen [::]:{{ include "mimir.serverHttpListenPort" . }};
              {{- end }}

              {{- if .Values.gateway.nginx.basicAuth.enabled }}
              auth_basic           "Mimir";
              auth_basic_user_file /etc/nginx/secrets/.htpasswd;
              {{- end }}

              if ($has_multiple_orgid_headers = 1) {
                  return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';
              }

              location = / {
                return 200 'OK';
                auth_basic off;
              }

              location = /ready {
                return 200 'OK';
                auth_basic off;
              }

              proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

              # Distributor endpoints
              location /distributor {
                set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }
              location = /api/v1/push {
                set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }
              location /otlp/v1/metrics {
                set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              # Alertmanager endpoints
              location {{ template "mimir.alertmanagerHttpPrefix" . }} {
                set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }
              location = /multitenant_alertmanager/status {
                set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }
              location = /api/v1/alerts {
                set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              # Ruler endpoints
              location {{ template "mimir.prometheusHttpPrefix" . }}/config/v1/rules {
                set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }
              location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/rules {
                set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/alerts {
                set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }
              location = /ruler/ring {
                set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              # Rest of {{ template "mimir.prometheusHttpPrefix" . }} goes to the query frontend
              location {{ template "mimir.prometheusHttpPrefix" . }} {
                set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              # Buildinfo endpoint can go to any component
              location = /api/v1/status/buildinfo {
                set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              # Compactor endpoint for uploading blocks
              location /api/v1/upload/block/ {
                set $compactor {{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                proxy_pass      http://$compactor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
              }

              {{- with .Values.gateway.nginx.config.serverSnippet }}
              {{ . | nindent 4 }}
              {{- end }}
            }
          }
        httpSnippet: ""
        logFormat: |-
          main '$remote_addr - $remote_user [$time_local]  $status '
                  '"$request" $body_bytes_sent "$http_referer" '
                  '"$http_user_agent" "$http_x_forwarded_for"';
        resolver: null
        serverSnippet: ""
      image:
        registry: docker.io
        repository: nginxinc/nginx-unprivileged
        tag: 1.25-alpine
      verboseLogging: true
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 15
      timeoutSeconds: 1
    replicas: 1
    resources: {}
    route:
      annotations: {}
      enabled: false
      host: '{{ .Release.Name }}.mimir.example.com'
      tls:
        termination: edge
    securityContext: {}
    service:
      annotations: {}
      clusterIP: null
      labels: {}
      legacyPort: 8080
      loadBalancerIP: null
      nameOverride: ""
      nodePort: null
      port: 80
      type: ClusterIP
    strategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  global:
    clusterDomain: cluster.local.
    dnsNamespace: kube-system
    dnsService: kube-dns
    extraEnv: []
    extraEnvFrom: []
    podAnnotations: {}
    podLabels: {}
  gr-aggr-cache:
    affinity: {}
    allocatedMemory: 8192
    annotations: {}
    connectionLimit: 16384
    enabled: true
    extraArgs: {}
    extraContainers: []
    initContainers: []
    maxItemMemory: 1
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
  gr-metricname-cache:
    affinity: {}
    allocatedMemory: 8192
    annotations: {}
    connectionLimit: 16384
    enabled: true
    extraArgs: {}
    extraContainers: []
    initContainers: []
    maxItemMemory: 1
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
  grafana-agent-operator:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
      seccompProfile:
        type: RuntimeDefault
  graphite:
    enabled: false
    querier:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - graphite-querier
              topologyKey: kubernetes.io/hostname
            weight: 100
      annotations: {}
      containerSecurityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      env: []
      extraArgs: {}
      extraContainers: []
      extraEnvFrom: []
      extraVolumeMounts: []
      extraVolumes: []
      initContainers: []
      jaegerReporterMaxQueueSize: null
      livenessProbe:
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 45
      nodeSelector: {}
      persistence:
        subPath: null
      podAnnotations: {}
      podDisruptionBudget:
        maxUnavailable: 1
      podLabels: {}
      priorityClassName: null
      readinessProbe:
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 45
      replicas: 2
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      schemasConfiguration:
        storageAggregations: |-
          [default]
          aggregationMethod = avg
          pattern = .*
          xFilesFactor = 0.1
        storageSchemas: |-
          [default]
          pattern = .*
          intervals = 0:1s
          retentions = 10s:8d,10min:1y
      securityContext: {}
      service:
        annotations: {}
        labels: {}
      strategy:
        rollingUpdate:
          maxSurge: 15%
          maxUnavailable: 0
        type: RollingUpdate
      terminationGracePeriodSeconds: 180
      tolerations: []
    write_proxy:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                  - graphite-write-proxy
              topologyKey: kubernetes.io/hostname
            weight: 100
      annotations: {}
      containerSecurityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      env: []
      extraArgs: {}
      extraContainers: []
      extraEnvFrom: []
      extraVolumeMounts: []
      extraVolumes: []
      initContainers: []
      jaegerReporterMaxQueueSize: null
      livenessProbe:
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 45
      nodeSelector: {}
      persistence:
        subPath: null
      podAnnotations: {}
      podDisruptionBudget:
        maxUnavailable: 1
      podLabels: {}
      priorityClassName: null
      readinessProbe:
        httpGet:
          path: /ready
          port: http-metrics
        initialDelaySeconds: 45
      replicas: 2
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext: {}
      service:
        annotations: {}
        labels: {}
      strategy:
        rollingUpdate:
          maxSurge: 15%
          maxUnavailable: 0
        type: RollingUpdate
      terminationGracePeriodSeconds: 180
      tolerations: []
  image:
    pullPolicy: IfNotPresent
    repository: grafana/mimir
    tag: 2.12.0
  index-cache:
    affinity: {}
    allocatedMemory: 2048
    annotations: {}
    connectionLimit: 16384
    enabled: false
    extraArgs: {}
    extraContainers: []
    extraExtendedOptions: ""
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    maxItemMemory: 5
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: {}
  ingester:
    native-histograms-ingestion-enabled: true
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: 1000
    nodeSelector: {}
    persistentVolume:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enableRetentionPolicy: false
      enabled: true
      size: 2Gi
      subPath: ""
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 60
    replicas: 4
    resources:
      requests:
        cpu: 20m
        memory: 512Mi
    schedulerName: ""
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    statefulSet:
      enabled: true
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 240
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    zoneAwareReplication:
      enabled: false
      maxUnavailable: 50
      migration:
        enabled: false
        excludeDefaultZone: false
        readPath: false
        replicas: 0
        scaleDownDefaultZone: false
        writePath: false
      topologyKey: null
      zones:
      - extraAffinity: {}
        name: zone-a
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-b
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-c
        nodeSelector: null
        storageClass: null
  ingress:
    annotations: {}
    enabled: false
    hosts:
    - mimir.example.com
    paths:
      alertmanager-headless:
      - path: /alertmanager
      - path: /multitenant_alertmanager/status
      - path: /api/v1/alerts
      compactor:
      - path: /api/v1/upload/block/
      distributor-headless:
      - path: /distributor
      - path: /api/v1/push
      - path: /otlp/v1/metrics
      query-frontend:
      - path: /prometheus
      - path: /api/v1/status/buildinfo
      ruler:
      - path: /prometheus/config/v1/rules
      - path: /prometheus/api/v1/rules
      - path: /prometheus/api/v1/alerts
  kedaAutoscaling:
    customHeaders: {}
    pollingInterval: 10
    prometheusAddress: ""
  kubeVersionOverride: null
  license:
    contents: NOTAVALIDLICENSE
    external: false
    secretName: '{{ include "mimir.resourceName" (dict "ctx" . "component" "license")
      }}'
  memcached:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    image:
      pullPolicy: IfNotPresent
      repository: memcached
      tag: 1.6.25-alpine
    podSecurityContext: {}
    priorityClassName: null
  memcachedExporter:
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabled: true
    extraArgs: {}
    image:
      pullPolicy: IfNotPresent
      repository: prom/memcached-exporter
      tag: v0.14.3
    resources:
      limits: {}
      requests: {}
  metaMonitoring:
    dashboards:
      annotations:
        k8s-sidecar-target-directory: /tmp/dashboards/Mimir Dashboards
      enabled: false
      labels:
        grafana_dashboard: "1"
    grafanaAgent:
      annotations: {}
      containerSecurityContext: null
      enabled: false
      imageRepo: null
      installOperator: false
      labels: {}
      logs:
        additionalClientConfigs: []
        enabled: true
        remote:
          auth:
            passwordSecretKey: ""
            passwordSecretName: ""
            tenantId: ""
            username: ""
          url: ""
      metrics:
        additionalRemoteWriteConfigs: []
        enabled: true
        remote:
          auth:
            passwordSecretKey: ""
            passwordSecretName: ""
            username: ""
          headers: {}
          url: ""
        scrapeInterval: 60s
        scrapeK8s:
          enabled: true
          kubeStateMetrics:
            labelSelectors:
              app.kubernetes.io/name: kube-state-metrics
            namespace: kube-system
            service:
              port: http-metrics
      namespace: ""
      podSecurityContext: null
    prometheusRule:
      annotations: {}
      enabled: false
      groups: []
      labels: {}
      mimirAlerts: false
      mimirRules: false
      namespace: null
    serviceMonitor:
      annotations: {}
      clusterLabel: ""
      enabled: false
      interval: null
      labels: {}
      metricRelabelings: []
      namespace: null
      namespaceSelector: null
      relabelings: []
      scheme: http
      scrapeTimeout: null
      tlsConfig: null
  metadata-cache:
    affinity: {}
    allocatedMemory: 512
    annotations: {}
    connectionLimit: 16384
    enabled: false
    extraArgs: {}
    extraContainers: []
    extraExtendedOptions: ""
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    maxItemMemory: 1
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: {}
  mimir:
    config: |
      usage_stats:
        installation_mode: helm

      activity_tracker:
        filepath: /active-query-tracker/activity.log

      {{- if .Values.enterprise.enabled }}
      admin_api:
        leader_election:
          enabled: true
          ring:
            kvstore:
              store: "memberlist"

      admin_client:
        storage:
        {{- if .Values.minio.enabled }}
          type: s3
          s3:
            access_key_id: {{ .Values.minio.rootUser }}
            bucket_name: enterprise-metrics-admin
            endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
            insecure: true
            secret_access_key: {{ .Values.minio.rootPassword }}
        {{- end }}
        {{- if (index .Values "admin-cache" "enabled") }}
          cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.adminCacheAddress" . }}
              max_item_size: {{ mul (index .Values "admin-cache").maxItemMemory 1024 1024 }}
        {{- end }}
      {{- end }}

      alertmanager:
        data_dir: /data
        enable_api: true
        external_url: /alertmanager
        {{- if .Values.alertmanager.zoneAwareReplication.enabled }}
        sharding_ring:
          zone_awareness_enabled: true
        {{- end }}
        {{- if .Values.alertmanager.fallbackConfig }}
        fallback_config_file: /configs/alertmanager_fallback_config.yaml
        {{- end }}

      {{- if .Values.minio.enabled }}
      alertmanager_storage:
        backend: s3
        s3:
          access_key_id: {{ .Values.minio.rootUser }}
          bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
          endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
          insecure: true
          secret_access_key: {{ .Values.minio.rootPassword }}
      {{- end }}

      {{- if .Values.enterprise.enabled }}
      auth:
        type: enterprise
      {{- end }}

      # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.
      blocks_storage:
        backend: s3
        bucket_store:
          {{- if index .Values "chunks-cache" "enabled" }}
          chunks_cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.chunksCacheAddress" . }}
              max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
              timeout: 750ms
              max_idle_connections: 150
          {{- end }}
          {{- if index .Values "index-cache" "enabled" }}
          index_cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.indexCacheAddress" . }}
              max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
              timeout: 750ms
              max_idle_connections: 150
          {{- end }}
          {{- if index .Values "metadata-cache" "enabled" }}
          metadata_cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.metadataCacheAddress" . }}
              max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
              max_idle_connections: 150
          {{- end }}
          sync_dir: /data/tsdb-sync
        {{- if .Values.minio.enabled }}
        s3:
          access_key_id: {{ .Values.minio.rootUser }}
          bucket_name: {{ include "mimir.minioBucketPrefix" . }}-tsdb
          endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
          insecure: true
          secret_access_key: {{ .Values.minio.rootPassword }}
        {{- end }}
        tsdb:
          dir: /data/tsdb
          head_compaction_interval: 15m
          wal_replay_concurrency: 3

      {{- if .Values.enterprise.enabled }}
      cluster_name: "{{ .Release.Name }}"
      {{- end }}

      compactor:
        compaction_interval: 30m
        deletion_delay: 2h
        max_closing_blocks_concurrency: 2
        max_opening_blocks_concurrency: 4
        symbols_flushers_concurrency: 4
        first_level_compaction_wait_period: 25m
        data_dir: "/data"
        sharding_ring:
          wait_stability_min_duration: 1m
          heartbeat_period: 1m
          heartbeat_timeout: 4m

      distributor:
        ring:
          heartbeat_period: 1m
          heartbeat_timeout: 4m

      frontend:
        parallelize_shardable_queries: true
        {{- if index .Values "results-cache" "enabled" }}
        results_cache:
          backend: memcached
          memcached:
            timeout: 500ms
            addresses: {{ include "mimir.resultsCacheAddress" . }}
            max_item_size: {{ mul (index .Values "results-cache").maxItemMemory 1024 1024 }}
        cache_results: true
        query_sharding_target_series_per_shard: 2500
        {{- end }}
        {{- if .Values.query_scheduler.enabled }}
        scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
        {{- end }}

      frontend_worker:
        grpc_client_config:
          max_send_msg_size: 419430400 # 400MiB
        {{- if .Values.query_scheduler.enabled }}
        scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
        {{- else }}
        frontend_address: {{ template "mimir.fullname" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
        {{- end }}

      {{- if and .Values.enterprise.enabled }}
      gateway:
        proxy:
          admin_api:
            url: http://{{ template "mimir.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          alertmanager:
            url: http://{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          compactor:
            url: http://{{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          default:
            url: http://{{ template "mimir.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          distributor:
            url: dns:///{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverGrpcListenPort" . }}
          ingester:
            url: http://{{ template "mimir.fullname" . }}-ingester-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          query_frontend:
            url: http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          ruler:
            url: http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          store_gateway:
            url: http://{{ template "mimir.fullname" . }}-store-gateway-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}
          graphite_write_proxy:
            url: http://{{ template "mimir.fullname" . }}-graphite-write-proxy.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          graphite_querier:
            url: http://{{ template "mimir.fullname" . }}-graphite-querier.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
          {{- end}}
      {{- end }}

      ingester:
        ring:
          final_sleep: 0s
          num_tokens: 512
          tokens_file_path: /data/tokens
          unregister_on_shutdown: false
          heartbeat_period: 2m
          heartbeat_timeout: 10m
          {{- if .Values.ingester.zoneAwareReplication.enabled }}
          zone_awareness_enabled: true
          {{- end }}

      ingester_client:
        grpc_client_config:
          max_recv_msg_size: 104857600
          max_send_msg_size: 104857600

      {{- if .Values.enterprise.enabled }}
      instrumentation:
        enabled: true
        distributor_client:
          address: dns:///{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverGrpcListenPort" . }}

      license:
        path: "/license/license.jwt"
      {{- end }}

      limits:
        # Limit queries to 500 days. You can override this on a per-tenant basis.
        max_total_query_length: 12000h
        # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
        # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
        max_query_parallelism: 240
        # Avoid caching results newer than 10m because some samples can be delayed
        # This presents caching incomplete results
        max_cache_freshness: 10m

      memberlist:
        abort_if_cluster_join_fails: false
        compression_enabled: false
        join_members:
        - dns+{{ include "mimir.fullname" . }}-gossip-ring.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.memberlistBindPort" . }}

      querier:
        # With query sharding we run more but smaller queries. We must strike a balance
        # which allows us to process more sharded queries in parallel when requested, but not overload
        # queriers during non-sharded queries.
        max_concurrent: 16

      query_scheduler:
        # Increase from default of 100 to account for queries created by query sharding
        max_outstanding_requests_per_tenant: 800

      ruler:
        alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
        enable_api: true
        rule_path: /data

      {{- if or (.Values.minio.enabled) (index .Values "metadata-cache" "enabled") }}
      ruler_storage:
        {{- if .Values.minio.enabled }}
        backend: s3
        s3:
          endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
          bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
          access_key_id: {{ .Values.minio.rootUser }}
          secret_access_key: {{ .Values.minio.rootPassword }}
          insecure: true
        {{- end }}
        {{- if index .Values "metadata-cache" "enabled" }}
        cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.metadataCacheAddress" . }}
            max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
        {{- end }}
      {{- end }}

      runtime_config:
        file: /var/{{ include "mimir.name" . }}/runtime.yaml

      store_gateway:
        sharding_ring:
          heartbeat_period: 1m
          heartbeat_timeout: 4m
          wait_stability_min_duration: 1m
          {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
          kvstore:
            prefix: multi-zone/
          {{- end }}
          tokens_file_path: /data/tokens
          unregister_on_shutdown: false
          {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
          zone_awareness_enabled: true
          {{- end }}

      {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}
      graphite:
        enabled: true

        write_proxy:
          distributor_client:
            address: dns:///{{ template "mimir.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" .  }}

        querier:
          remote_read:
            query_address: http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" .  }}/prometheus
          proxy_bad_requests: false

          schemas:
            default_storage_schemas_file: /etc/graphite-proxy/storage-schemas.conf
            default_storage_aggregations_file: /etc/graphite-proxy/storage-aggregations.conf
          aggregation_cache:
            memcached:
              addresses: dnssrvnoa+{{ template "mimir.fullname" . }}-gr-aggr-cache.{{ .Release.Namespace}}.svc:11211
              timeout: 1s
          metric_name_cache:
            memcached:
              addresses: dnssrvnoa+{{ template "mimir.fullname" . }}-gr-metricname-cache.{{ .Release.Namespace}}.svc:11211
              timeout: 1s
      {{- end}}
    structuredConfig: {}
  minio:
    additionalAnnotations: {}
    additionalLabels: {}
    affinity: {}
    bucketRoot: ""
    buckets:
    - name: mimir-tsdb
      policy: none
      purge: false
    - name: mimir-ruler
      policy: none
      purge: false
    - name: enterprise-metrics-tsdb
      policy: none
      purge: false
    - name: enterprise-metrics-admin
      policy: none
      purge: false
    - name: enterprise-metrics-ruler
      policy: none
      purge: false
    certsPath: /etc/minio/certs/
    clusterDomain: cluster.local
    configPathmc: /tmp/minio/mc/
    consoleIngress:
      annotations: {}
      enabled: false
      hosts:
      - console.minio-example.local
      ingressClassName: null
      labels: {}
      path: /
      tls: []
    consoleService:
      annotations: {}
      clusterIP: null
      externalIPs: []
      loadBalancerIP: null
      nodePort: 32001
      port: "9001"
      type: ClusterIP
    customCommandJob:
      exitCommand: ""
      resources:
        requests:
          memory: 128Mi
      securityContext:
        enabled: false
        runAsGroup: 1000
        runAsUser: 1000
    customCommands: null
    deploymentUpdate:
      maxSurge: 100%
      maxUnavailable: 0
      type: RollingUpdate
    drivesPerNode: 1
    enabled: true
    environment: null
    etcd:
      clientCert: ""
      clientCertKey: ""
      corednsPathPrefix: ""
      endpoints: []
      pathPrefix: ""
    existingSecret: ""
    extraArgs: []
    extraContainers: []
    extraSecret: null
    extraVolumeMounts: []
    extraVolumes: []
    fullnameOverride: ""
    global:
      clusterDomain: cluster.local.
      dnsNamespace: kube-system
      dnsService: kube-dns
      extraEnv: []
      extraEnvFrom: []
      podAnnotations: {}
      podLabels: {}
    ignoreChartChecksums: false
    image:
      pullPolicy: IfNotPresent
      repository: quay.io/minio/minio
      tag: RELEASE.2023-09-30T07-02-29Z
    imagePullSecrets: []
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - minio-example.local
      ingressClassName: null
      labels: {}
      path: /
      tls: []
    makeBucketJob:
      exitCommand: ""
      resources:
        requests:
          memory: 128Mi
      securityContext:
        enabled: false
        runAsGroup: 1000
        runAsUser: 1000
    makePolicyJob:
      exitCommand: ""
      resources:
        requests:
          memory: 128Mi
      securityContext:
        enabled: false
        runAsGroup: 1000
        runAsUser: 1000
    makeServiceAccountJob:
      exitCommand: ""
      resources:
        requests:
          memory: 128Mi
      securityContext:
        enabled: false
        runAsGroup: 1000
        runAsUser: 1000
    makeUserJob:
      exitCommand: ""
      resources:
        requests:
          memory: 128Mi
      securityContext:
        enabled: false
        runAsGroup: 1000
        runAsUser: 1000
    mcImage:
      pullPolicy: IfNotPresent
      repository: quay.io/minio/mc
      tag: RELEASE.2023-09-29T16-41-22Z
    metrics:
      serviceMonitor:
        additionalLabels: {}
        annotations: {}
        enabled: false
        includeNode: false
        interval: null
        namespace: null
        public: true
        relabelConfigs: {}
        relabelConfigsCluster: {}
        scrapeTimeout: null
    minioAPIPort: "9000"
    minioConsolePort: "9001"
    mode: standalone
    mountPath: /export
    nameOverride: ""
    networkPolicy:
      allowExternal: true
      enabled: false
    nodeSelector: {}
    oidc:
      claimName: policy
      claimPrefix: ""
      clientId: minio
      clientSecret: ""
      comment: ""
      configUrl: https://identity-provider-url/.well-known/openid-configuration
      enabled: false
      existingClientSecretKey: ""
      existingClientSecretName: ""
      redirectUri: https://console-endpoint-url/oauth_callback
      scopes: openid,profile,email
    persistence:
      accessMode: ReadWriteOnce
      annotations: {}
      enabled: true
      existingClaim: ""
      size: 5Gi
      storageClass: ""
      subPath: ""
      volumeName: ""
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      maxUnavailable: 1
    podLabels: {}
    policies: []
    pools: 1
    postJob:
      affinity: {}
      annotations: {}
      nodeSelector: {}
      podAnnotations: {}
      securityContext:
        enabled: false
        fsGroup: 1000
        runAsGroup: 1000
        runAsUser: 1000
      tolerations: []
    priorityClassName: ""
    replicas: 16
    resources:
      requests:
        cpu: 20m
        memory: 128Mi
    rootPassword: supersecret
    rootUser: grafana-mimir
    runtimeClassName: ""
    securityContext:
      enabled: true
      fsGroup: 1000
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 1000
      runAsUser: 1000
    service:
      annotations: {}
      clusterIP: null
      externalIPs: []
      loadBalancerIP: null
      nodePort: 32000
      port: "9000"
      type: ClusterIP
    serviceAccount:
      create: true
      name: minio-sa
    statefulSetUpdate:
      updateStrategy: RollingUpdate
    svcaccts: []
    tls:
      certSecret: ""
      enabled: false
      privateKey: private.key
      publicCrt: public.crt
    tolerations: []
    topologySpreadConstraints: []
    trustedCertsSecret: ""
    users:
    - accessKey: console
      policy: consoleAdmin
      secretKey: console123
  nameOverride: null
  nginx:
    affinity: ""
    annotations: {}
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    basicAuth:
      enabled: false
      existingSecret: null
      htpasswd: '{{ htpasswd (required "''nginx.basicAuth.username'' is required"
        .Values.nginx.basicAuth.username) (required "''nginx.basicAuth.password''
        is required" .Values.nginx.basicAuth.password) }}'
      password: null
      username: null
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    deploymentStrategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    enabled: true
    extraArgs: {}
    extraContainers: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    image:
      pullPolicy: IfNotPresent
      registry: docker.io
      repository: nginxinc/nginx-unprivileged
      tag: 1.25-alpine
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - host: nginx.mimir.example.com
        paths:
        - path: /
      tls:
      - hosts:
        - nginx.mimir.example.com
        secretName: mimir-nginx-tls
    nginxConfig:
      accessLogEnabled: true
      errorLogLevel: error
      file: |
        worker_processes  5;  ## Default: 1
        error_log  /dev/stderr {{ .Values.nginx.nginxConfig.errorLogLevel }};
        pid        /tmp/nginx.pid;
        worker_rlimit_nofile 8192;

        events {
          worker_connections  4096;  ## Default: 1024
        }

        http {
          client_body_temp_path /tmp/client_temp;
          proxy_temp_path       /tmp/proxy_temp_path;
          fastcgi_temp_path     /tmp/fastcgi_temp;
          uwsgi_temp_path       /tmp/uwsgi_temp;
          scgi_temp_path        /tmp/scgi_temp;

          default_type application/octet-stream;
          log_format   {{ .Values.nginx.nginxConfig.logFormat }}

          {{- if .Values.nginx.verboseLogging }}
          access_log   /dev/stderr  main;
          {{- else }}

          map $status $loggable {
            ~^[23]  0;
            default 1;
          }
          access_log   {{ .Values.nginx.nginxConfig.accessLogEnabled | ternary "/dev/stderr  main  if=$loggable;" "off;" }}
          {{- end }}

          sendfile           on;
          tcp_nopush         on;
          proxy_http_version 1.1;

          {{- if .Values.nginx.nginxConfig.resolver }}
          resolver {{ .Values.nginx.nginxConfig.resolver }};
          {{- else }}
          resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
          {{- end }}

          {{- with .Values.nginx.nginxConfig.httpSnippet }}
          {{ . | nindent 2 }}
          {{- end }}

          # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
          map $http_x_scope_orgid $ensured_x_scope_orgid {
            default $http_x_scope_orgid;
            "" "{{ include "mimir.noAuthTenant" . }}";
          }

          map $http_x_scope_orgid $has_multiple_orgid_headers {
            default 0;
            "~^.+,.+$" 1;
          }

          proxy_read_timeout 300;
          server {
            listen 8080;
            listen [::]:8080;

            {{- if .Values.nginx.basicAuth.enabled }}
            auth_basic           "Mimir";
            auth_basic_user_file /etc/nginx/secrets/.htpasswd;
            {{- end }}

            if ($has_multiple_orgid_headers = 1) {
                return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';
            }

            location = / {
              return 200 'OK';
              auth_basic off;
            }

            proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

            # Distributor endpoints
            location /distributor {
              set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /api/v1/push {
              set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location /otlp/v1/metrics {
              set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            # Alertmanager endpoints
            location {{ template "mimir.alertmanagerHttpPrefix" . }} {
              set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /multitenant_alertmanager/status {
              set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /api/v1/alerts {
              set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            # Ruler endpoints
            location {{ template "mimir.prometheusHttpPrefix" . }}/config/v1/rules {
              set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/rules {
              set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/alerts {
              set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }
            location = /ruler/ring {
              set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            # Rest of {{ template "mimir.prometheusHttpPrefix" . }} goes to the query frontend
            location {{ template "mimir.prometheusHttpPrefix" . }} {
              set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            # Buildinfo endpoint can go to any component
            location = /api/v1/status/buildinfo {
              set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            # Compactor endpoint for uploading blocks
            location /api/v1/upload/block/ {
              set $compactor {{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass      http://$compactor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
            }

            {{- with .Values.nginx.nginxConfig.serverSnippet }}
            {{ . | nindent 4 }}
            {{- end }}
          }
        }
      httpSnippet: ""
      logFormat: |-
        main '$remote_addr - $remote_user [$time_local]  $status '
                '"$request" $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';
      resolver: null
      serverSnippet: ""
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podSecurityContext: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /
        port: http-metric
      initialDelaySeconds: 15
      timeoutSeconds: 1
    replicas: 1
    resources: {}
    route:
      annotations: {}
      enabled: false
      host: nginx.mimir.example.com
      tls:
        termination: edge
    service:
      annotations: {}
      clusterIP: null
      labels: {}
      loadBalancerIP: null
      nodePort: null
      port: 80
      type: ClusterIP
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    verboseLogging: true
  overrides_exporter:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabled: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: null
    livenessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 128Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: {}
  querier:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: 5000
    kedaAutoscaling:
      behavior:
        scaleDown:
          policies:
          - periodSeconds: 120
            type: Percent
            value: 10
          stabilizationWindowSeconds: 600
        scaleUp:
          policies:
          - periodSeconds: 120
            type: Percent
            value: 50
          - periodSeconds: 120
            type: Pods
            value: 15
          stabilizationWindowSeconds: 60
      enabled: false
      maxReplicaCount: 10
      minReplicaCount: 1
      preserveReplicas: false
      querySchedulerInflightRequestsThreshold: 12
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 128Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 180
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  query_frontend:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: 5000
    kedaAutoscaling:
      behavior:
        scaleDown:
          policies:
          - periodSeconds: 60
            type: Percent
            value: 10
      enabled: false
      maxReplicaCount: 10
      minReplicaCount: 1
      preserveReplicas: false
      targetCPUUtilizationPercentage: 75
      targetMemoryUtilizationPercentage: 100
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 128Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 15%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 390
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  query_scheduler:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabled: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: null
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 128Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 180
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  rbac:
    create: true
    forcePSPOnKubernetes124: false
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
      seccompProfile:
        type: RuntimeDefault
    podSecurityPolicy:
      additionalVolumes: []
      allowPrivilegeEscalation: false
      fsGroup:
        ranges:
        - max: 65535
          min: 1
        rule: MustRunAs
      hostIPC: false
      hostNetwork: false
      hostPID: false
      privileged: false
      readOnlyRootFilesystem: true
      runAsUser:
        rule: MustRunAsNonRoot
      seLinux:
        rule: RunAsAny
      seccompProfile: runtime/default
      supplementalGroups:
        ranges:
        - max: 65535
          min: 1
        rule: MustRunAs
    type: psp
  results-cache:
    affinity: {}
    allocatedMemory: 512
    annotations: {}
    connectionLimit: 16384
    enabled: false
    extraArgs: {}
    extraContainers: []
    extraExtendedOptions: ""
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    maxItemMemory: 5
    nodeSelector: {}
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: Parallel
    port: 11211
    priorityClassName: null
    replicas: 1
    resources: null
    service:
      annotations: {}
      labels: {}
    statefulStrategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: {}
  rollout_operator:
    affinity: {}
    enabled: true
    fullnameOverride: ""
    global:
      clusterDomain: cluster.local.
      dnsNamespace: kube-system
      dnsService: kube-dns
      extraEnv: []
      extraEnvFrom: []
      podAnnotations: {}
      podLabels: {}
    hostAliases: []
    image:
      pullPolicy: IfNotPresent
      repository: grafana/rollout-operator
      tag: ""
    imagePullSecrets: []
    minReadySeconds: 10
    nameOverride: ""
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
      seccompProfile:
        type: RuntimeDefault
    priorityClassName: ""
    resources:
      limits:
        memory: 200Mi
      requests:
        cpu: 20m
        memory: 100Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    serviceAccount:
      annotations: {}
      create: true
      name: ""
    serviceMonitor:
      annotations: {}
      enabled: false
      interval: null
      labels: {}
      namespace: null
      namespaceSelector: {}
      relabelings: []
      scrapeTimeout: null
    tolerations: []
  ruler:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enabled: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: 1000
    kedaAutoscaling:
      behavior:
        scaleDown:
          policies:
          - periodSeconds: 600
            type: Percent
            value: 10
      enabled: false
      maxReplicaCount: 10
      minReplicaCount: 1
      preserveReplicas: false
      targetCPUUtilizationPercentage: 100
      targetMemoryUtilizationPercentage: 100
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 128Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    serviceAccount:
      annotations: {}
      create: false
      labels: {}
      name: ""
    strategy:
      rollingUpdate:
        maxSurge: 50%
        maxUnavailable: 0
      type: RollingUpdate
    terminationGracePeriodSeconds: 180
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
  runtimeConfig: {}
  serviceAccount:
    annotations: {}
    create: true
    labels: {}
    name: null
  smoke_test:
    annotations: {}
    env: []
    extraArgs: {}
    extraEnvFrom: []
    image:
      pullPolicy: IfNotPresent
      repository: grafana/mimir-continuous-test
      tag: 2.12.0
    initContainers: []
    priorityClassName: null
    tenantId: ""
  store_gateway:
    affinity: {}
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    initContainers: []
    jaegerReporterMaxQueueSize: 1000
    nodeSelector: {}
    persistentVolume:
      accessModes:
      - ReadWriteOnce
      annotations: {}
      enableRetentionPolicy: false
      enabled: true
      size: 2Gi
      subPath: ""
      whenDeleted: Retain
      whenScaled: Retain
    podAnnotations: {}
    podDisruptionBudget:
      maxUnavailable: 1
    podLabels: {}
    podManagementPolicy: OrderedReady
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 60
    replicas: 1
    resources:
      requests:
        cpu: 20m
        memory: 512Mi
    schedulerName: ""
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      type: RollingUpdate
    terminationGracePeriodSeconds: 240
    tolerations: []
    topologySpreadConstraints:
      maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
    zoneAwareReplication:
      enabled: false
      maxUnavailable: 50
      migration:
        enabled: false
        readPath: false
      topologyKey: null
      zones:
      - extraAffinity: {}
        name: zone-a
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-b
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-c
        nodeSelector: null
        storageClass: null
  tokengenJob:
    annotations: {}
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
    enable: true
    env: []
    extraArgs: {}
    extraEnvFrom: []
    initContainers: []
    priorityClassName: null
    securityContext: {}
  useExternalConfig: false
  vaultAgent:
    caCertPath: ""
    clientCertPath: ""
    clientKeyPath: ""
    enabled: false
    roleName: ""
    serverCertPath: ""
    serverKeyPath: ""
tempo:
  adminApi:
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "admin-api") | nindent 12 }}
              topologyKey: kubernetes.io/hostname
          - weight: 75
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "admin-api") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    annotations: {}
    containerSecurityContext:
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    initContainers: []
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget: {}
    podLabels: {}
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
    securityContext: {}
    service:
      annotations: {}
      labels: {}
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "admin-api") | nindent 6 }}
  cache:
    caches:
    - memcached:
        consistent_hash: true
        host: '{{ include "tempo.fullname" . }}-memcached'
        service: memcached-client
        timeout: 500ms
      roles:
      - parquet-footer
      - bloom
      - frontend-search
  compactor:
    config:
      compaction:
        block_retention: 48h
        compacted_block_retention: 1h
        compaction_cycle: 30s
        compaction_window: 1h
        max_block_bytes: 107374182400
        max_compaction_objects: 6000000
        max_time_per_tenant: 5m
        retention_concurrency: 10
        v2_in_buffer_bytes: 5242880
        v2_out_buffer_bytes: 20971520
        v2_prefetch_traces_count: 1000
    dnsConfigOverides:
      dnsConfig:
        options:
        - name: ndots
          value: "3"
      enabled: false
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    service:
      annotations: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
  config: |
    multitenancy_enabled: {{ .Values.multitenancyEnabled }}

    usage_report:
      reporting_enabled: {{ .Values.reportingEnabled }}

    {{- if .Values.enterprise.enabled }}
    license:
      path: "/license/license.jwt"

    admin_api:
      leader_election:
        enabled: true
        ring:
          kvstore:
            store: "memberlist"

    auth:
      type: enterprise

    http_api_prefix: {{get .Values.tempo.structuredConfig "http_api_prefix"}}

    admin_client:
      storage:
        backend: {{.Values.storage.admin.backend}}
        {{- if eq .Values.storage.admin.backend "s3"}}
        s3:
          {{- toYaml .Values.storage.admin.s3 | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "gcs"}}
        gcs:
          {{- toYaml .Values.storage.admin.gcs | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "azure"}}
        azure:
          {{- toYaml .Values.storage.admin.azure | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "swift"}}
        swift:
          {{- toYaml .Values.storage.admin.swift | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "filesystem"}}
        filesystem:
          {{- toYaml .Values.storage.admin.filesystem | nindent 6}}
        {{- end}}
    {{- end }}

    {{- if and .Values.enterprise.enabled .Values.enterpriseGateway.useDefaultProxyURLs }}
    gateway:
      proxy:
        admin_api:
          url: http://{{ template "tempo.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        compactor:
          url: http://{{ template "tempo.fullname" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        default:
          url: http://{{ template "tempo.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        distributor:
          url: http://{{ template "tempo.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
          otlp/grpc:
            url: h2c://{{ template "tempo.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:4317
          otlp/http:
            url: http://{{ template "tempo.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:4318
        ingester:
          url: http://{{ template "tempo.fullname" . }}-ingester.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        querier:
          url: http://{{ template "tempo.fullname" . }}-querier.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        query_frontend:
          url: http://{{ template "tempo.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}{{get .Values.tempo.structuredConfig "http_api_prefix"}}
    {{else}}
    {{- if and .Values.enterprise.enabled .Values.enterpriseGateway.proxy }}
    gateway:
      proxy: {{- toYaml .Values.enterpriseGateway.proxy | nindent 6 }}
    {{- end }}
    {{- end }}

    compactor:
      compaction:
        block_retention: {{ .Values.compactor.config.compaction.block_retention }}
        compacted_block_retention: {{ .Values.compactor.config.compaction.compacted_block_retention }}
        compaction_window: {{ .Values.compactor.config.compaction.compaction_window }}
        v2_in_buffer_bytes: {{ .Values.compactor.config.compaction.v2_in_buffer_bytes }}
        v2_out_buffer_bytes: {{ .Values.compactor.config.compaction.v2_out_buffer_bytes }}
        max_compaction_objects: {{ .Values.compactor.config.compaction.max_compaction_objects }}
        max_block_bytes: {{ .Values.compactor.config.compaction.max_block_bytes }}
        retention_concurrency: {{ .Values.compactor.config.compaction.retention_concurrency }}
        v2_prefetch_traces_count: {{ .Values.compactor.config.compaction.v2_prefetch_traces_count }}
        max_time_per_tenant: {{ .Values.compactor.config.compaction.max_time_per_tenant }}
        compaction_cycle: {{ .Values.compactor.config.compaction.compaction_cycle }}
      ring:
        kvstore:
          store: memberlist
    {{- if and .Values.enterprise.enabled .Values.enterpriseFederationFrontend.enabled }}
    federation:
      proxy_targets:
        {{- toYaml .Values.enterpriseFederationFrontend.proxy_targets | nindent 6 }}
    {{- end }}
    {{- if .Values.metricsGenerator.enabled }}
    metrics_generator:
      ring:
        kvstore:
          store: memberlist
      processor:
        {{- toYaml .Values.metricsGenerator.config.processor | nindent 6 }}
      storage:
        {{- toYaml .Values.metricsGenerator.config.storage | nindent 6 }}
      traces_storage:
        {{- toYaml .Values.metricsGenerator.config.traces_storage | nindent 6 }}
      registry:
        {{- toYaml .Values.metricsGenerator.config.registry | nindent 6 }}
      metrics_ingestion_time_range_slack: {{ .Values.metricsGenerator.config.metrics_ingestion_time_range_slack }}
    {{- end }}
    distributor:
      ring:
        kvstore:
          store: memberlist
      receivers:
        {{- if  or (.Values.traces.jaeger.thriftCompact.enabled) (.Values.traces.jaeger.thriftBinary.enabled) (.Values.traces.jaeger.thriftHttp.enabled) (.Values.traces.jaeger.grpc.enabled) }}
        jaeger:
          protocols:
            {{- if .Values.traces.jaeger.thriftCompact.enabled }}
            thrift_compact:
              {{- $mergedJaegerThriftCompactConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:6831") .Values.traces.jaeger.thriftCompact.receiverConfig }}
              {{- toYaml $mergedJaegerThriftCompactConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.jaeger.thriftBinary.enabled }}
            thrift_binary:
              {{- $mergedJaegerThriftBinaryConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:6832") .Values.traces.jaeger.thriftBinary.receiverConfig }}
              {{- toYaml $mergedJaegerThriftBinaryConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.jaeger.thriftHttp.enabled }}
            thrift_http:
              {{- $mergedJaegerThriftHttpConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:14268") .Values.traces.jaeger.thriftHttp.receiverConfig }}
              {{- toYaml $mergedJaegerThriftHttpConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.jaeger.grpc.enabled }}
            grpc:
              {{- $mergedJaegerGrpcConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:14250") .Values.traces.jaeger.grpc.receiverConfig }}
              {{- toYaml $mergedJaegerGrpcConfig | nindent 10 }}
            {{- end }}
        {{- end }}
        {{- if .Values.traces.zipkin.enabled }}
        zipkin:
          {{- $mergedZipkinReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:9411") .Values.traces.zipkin.receiverConfig }}
          {{- toYaml $mergedZipkinReceiverConfig | nindent 6 }}
        {{- end }}
        {{- if or (.Values.traces.otlp.http.enabled) (.Values.traces.otlp.grpc.enabled) }}
        otlp:
          protocols:
            {{- if .Values.traces.otlp.http.enabled }}
            http:
              {{- $mergedOtlpHttpReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:4318") .Values.traces.otlp.http.receiverConfig }}
              {{- toYaml $mergedOtlpHttpReceiverConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.otlp.grpc.enabled }}
            grpc:
              {{- $mergedOtlpGrpcReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:4317") .Values.traces.otlp.grpc.receiverConfig }}
              {{- toYaml $mergedOtlpGrpcReceiverConfig | nindent 10 }}
            {{- end }}
        {{- end }}
        {{- if .Values.traces.opencensus.enabled }}
        opencensus:
          {{- $mergedOpencensusReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:55678") .Values.traces.opencensus.receiverConfig }}
          {{- toYaml $mergedOpencensusReceiverConfig | nindent 6 }}
        {{- end }}
        {{- if .Values.traces.kafka }}
        kafka:
          {{- toYaml .Values.traces.kafka | nindent 6 }}
        {{- end }}
      {{- if or .Values.distributor.config.log_received_traces .Values.distributor.config.log_received_spans.enabled }}
      log_received_spans:
        enabled: {{ or .Values.distributor.config.log_received_traces .Values.distributor.config.log_received_spans.enabled }}
        include_all_attributes: {{ .Values.distributor.config.log_received_spans.include_all_attributes }}
        filter_by_status_error: {{ .Values.distributor.config.log_received_spans.filter_by_status_error }}
      {{- end }}
      {{- if .Values.distributor.config.extend_writes }}
      extend_writes: {{ .Values.distributor.config.extend_writes }}
      {{- end }}
    querier:
      frontend_worker:
        frontend_address: {{ include "tempo.resourceName" (dict "ctx" . "component" "query-frontend-discovery") }}:9095
        {{- if .Values.querier.config.frontend_worker.grpc_client_config }}
        grpc_client_config:
          {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}
        {{- end }}
      trace_by_id:
        query_timeout: {{ .Values.querier.config.trace_by_id.query_timeout }}
      search:
        external_endpoints: {{- toYaml .Values.querier.config.search.external_endpoints | nindent 6 }}
        query_timeout: {{ .Values.querier.config.search.query_timeout }}
        prefer_self: {{ .Values.querier.config.search.prefer_self }}
        external_hedge_requests_at: {{ .Values.querier.config.search.external_hedge_requests_at }}
        external_hedge_requests_up_to: {{ .Values.querier.config.search.external_hedge_requests_up_to }}
        external_backend: {{ .Values.querier.config.search.external_backend }}
        {{- if .Values.querier.config.search.google_cloud_run }}
        google_cloud_run:
          {{- toYaml .Values.querier.config.search.google_cloud_run | nindent 6 }}
        {{- end }}
      max_concurrent_queries: {{ .Values.querier.config.max_concurrent_queries }}
    query_frontend:
      max_outstanding_per_tenant: {{ .Values.queryFrontend.config.max_outstanding_per_tenant }}
      max_retries: {{ .Values.queryFrontend.config.max_retries }}
      search:
        target_bytes_per_job: {{ .Values.queryFrontend.config.search.target_bytes_per_job }}
        concurrent_jobs: {{ .Values.queryFrontend.config.search.concurrent_jobs }}
      trace_by_id:
        query_shards: {{ .Values.queryFrontend.config.trace_by_id.query_shards }}

    ingester:
      lifecycler:
        ring:
          replication_factor: {{ .Values.ingester.config.replication_factor }}
          {{- if .Values.ingester.zoneAwareReplication.enabled }}
          zone_awareness_enabled: true
          {{- end }}
          kvstore:
            store: memberlist
        tokens_file_path: /var/tempo/tokens.json
      {{- if .Values.ingester.config.trace_idle_period }}
      trace_idle_period: {{ .Values.ingester.config.trace_idle_period }}
      {{- end }}
      {{- if .Values.ingester.config.flush_check_period }}
      flush_check_period: {{ .Values.ingester.config.flush_check_period }}
      {{- end }}
      {{- if .Values.ingester.config.max_block_bytes }}
      max_block_bytes: {{ .Values.ingester.config.max_block_bytes }}
      {{- end }}
      {{- if .Values.ingester.config.max_block_duration }}
      max_block_duration: {{ .Values.ingester.config.max_block_duration }}
      {{- end }}
      {{- if .Values.ingester.config.complete_block_timeout }}
      complete_block_timeout: {{ .Values.ingester.config.complete_block_timeout }}
      {{- end }}
      {{- if .Values.ingester.config.flush_all_on_shutdown }}
      flush_all_on_shutdown: {{ .Values.ingester.config.flush_all_on_shutdown }}
      {{- end }}
    memberlist:
      {{- with .Values.memberlist }}
        {{- toYaml . | nindent 2 }}
      {{- end }}
      join_members:
        - dns+{{ include "tempo.fullname" . }}-gossip-ring:{{ .Values.memberlist.bind_port }}
    overrides:
      {{- toYaml .Values.global_overrides | nindent 2 }}
    server:
      http_listen_port: {{ .Values.server.httpListenPort }}
      log_level: {{ .Values.server.logLevel }}
      log_format: {{ .Values.server.logFormat }}
      grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}
      grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}
      http_server_read_timeout: {{ .Values.server.http_server_read_timeout }}
      http_server_write_timeout: {{ .Values.server.http_server_write_timeout }}
    cache:
    {{- toYaml .Values.cache | nindent 2}}
    storage:
      trace:
        {{- if .Values.storage.trace.block.version }}
        block:
          version: {{.Values.storage.trace.block.version}}
          {{- if .Values.storage.trace.block.dedicated_columns}}
          parquet_dedicated_columns:
            {{ .Values.storage.trace.block.dedicated_columns | toYaml | nindent 8}}
          {{- end }}
        {{- end }}
        pool:
          max_workers: {{ .Values.storage.trace.pool.max_workers }}
          queue_depth: {{ .Values.storage.trace.pool.queue_depth }}
        backend: {{.Values.storage.trace.backend}}
        {{- if eq .Values.storage.trace.backend "s3"}}
        s3:
          {{- toYaml .Values.storage.trace.s3 | nindent 6}}
        {{- end }}
        {{- if eq .Values.storage.trace.backend "gcs"}}
        gcs:
          {{- toYaml .Values.storage.trace.gcs | nindent 6}}
        {{- end }}
        {{- if eq .Values.storage.trace.backend "azure"}}
        azure:
          {{- toYaml .Values.storage.trace.azure | nindent 6}}
        {{- end }}
        blocklist_poll: 5m
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal
  configStorageType: ConfigMap
  distributor:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    appProtocol:
      grpc: null
    autoscaling:
      behavior: {}
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    config:
      extend_writes: null
      log_received_spans:
        enabled: false
        filter_by_status_error: false
        include_all_attributes: false
      log_received_traces: null
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    service:
      annotations: {}
      labels: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      type: ClusterIP
    serviceDiscovery:
      annotations: {}
      labels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 6 }}
  enabled: true
  enterprise:
    enabled: false
    image:
      repository: grafana/enterprise-traces
      tag: v2.4.0
  enterpriseFederationFrontend:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "federation-frontend") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "federation-frontend") | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    autoscaling:
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    enabled: false
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    proxy_targets: []
    replicas: 1
    resources: {}
    service:
      annotations: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      port: 3100
      type: ClusterIP
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: failure-domain.beta.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "federation-frontend") | nindent 6 }}
  enterpriseGateway:
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "enterprise-gateway") | nindent 12 }}
              topologyKey: kubernetes.io/hostname
          - weight: 75
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "enterprise-gateway") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    annotations: {}
    containerSecurityContext:
      readOnlyRootFilesystem: true
    env: []
    extraArgs: {}
    extraContainers: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - host: gateway.gem.example.com
        paths:
        - path: /
      tls:
      - hosts:
        - gateway.gem.example.com
        secretName: gem-gateway-tls
    initContainers: []
    nodeSelector: {}
    persistence:
      subPath: null
    podAnnotations: {}
    podDisruptionBudget: {}
    podLabels: {}
    proxy: {}
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45
    replicas: 1
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
    securityContext: {}
    service:
      annotations: {}
      clusterIP: null
      labels: {}
      loadBalancerIP: null
      port: null
      type: ClusterIP
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
    terminationGracePeriodSeconds: 60
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "enterprise-gateway") | nindent 6 }}
    useDefaultProxyURLs: true
  externalConfigSecretName: '{{ include "tempo.resourceName" (dict "ctx" . "component"
    "config") }}'
  externalConfigVersion: "0"
  externalRuntimeConfigName: '{{ include "tempo.resourceName" (dict "ctx" . "component"
    "runtime") }}'
  extraObjects: []
  fullnameOverride: ""
  gateway:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    autoscaling:
      behavior: {}
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    basicAuth:
      enabled: false
      existingSecret: null
      htpasswd: '{{ htpasswd (required "''gateway.basicAuth.username'' is required"
        .Values.gateway.basicAuth.username) (required "''gateway.basicAuth.password''
        is required" .Values.gateway.basicAuth.password) }}'
      password: null
      username: null
    enabled: false
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: null
      repository: nginxinc/nginx-unprivileged
      tag: 1.19-alpine
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - host: gateway.tempo.example.com
        paths:
        - path: /
      labels: {}
      tls:
      - hosts:
        - gateway.tempo.example.com
        secretName: tempo-gateway-tls
    nginxConfig:
      file: |
        worker_processes  5;  ## Default: 1
        error_log  /dev/stderr;
        pid        /tmp/nginx.pid;
        worker_rlimit_nofile 8192;

        events {
          worker_connections  4096;  ## Default: 1024
        }

        http {
          client_body_temp_path /tmp/client_temp;
          proxy_temp_path       /tmp/proxy_temp_path;
          fastcgi_temp_path     /tmp/fastcgi_temp;
          uwsgi_temp_path       /tmp/uwsgi_temp;
          scgi_temp_path        /tmp/scgi_temp;

          proxy_http_version    1.1;

          default_type application/octet-stream;
          log_format   {{ .Values.gateway.nginxConfig.logFormat }}

          {{- if .Values.gateway.verboseLogging }}
          access_log   /dev/stderr  main;
          {{- else }}

          map $status $loggable {
            ~^[23]  0;
            default 1;
          }
          access_log   /dev/stderr  main  if=$loggable;
          {{- end }}

          sendfile     on;
          tcp_nopush   on;
          {{- if .Values.gateway.nginxConfig.resolver }}
          resolver {{ .Values.gateway.nginxConfig.resolver }};
          {{- else }}
          resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
          {{- end }}

          {{- with .Values.gateway.nginxConfig.httpSnippet }}
          {{ . | nindent 2 }}
          {{- end }}

          server {
            listen             8080;

            {{- if .Values.gateway.basicAuth.enabled }}
            auth_basic           "Tempo";
            auth_basic_user_file /etc/nginx/secrets/.htpasswd;
            {{- end }}

            location = / {
              return 200 'OK';
              auth_basic off;
            }

            location = /jaeger/api/traces {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:14268/api/traces;
            }

            location = /zipkin/spans {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:9411/spans;
            }

            location = /v1/traces {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:4318/v1/traces;
            }

            location = /otlp/v1/traces {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:4318/v1/traces;
            }

            location ^~ /api {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "query-frontend") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            location = /flush {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "ingester") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            location = /shutdown {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "ingester") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            location = /distributor/ring {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            location = /ingester/ring {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            location = /compactor/ring {
              proxy_pass       http://{{ include "tempo.resourceName" (dict "ctx" . "component" "compactor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            }

            {{- with .Values.gateway.nginxConfig.serverSnippet }}
            {{ . | nindent 4 }}
            {{- end }}
          }
        }
      httpSnippet: ""
      logFormat: |-
        main '$remote_addr - $remote_user [$time_local]  $status '
                '"$request" $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';
      resolver: ""
      serverSnippet: ""
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    readinessProbe:
      httpGet:
        path: /
        port: http-metrics
      initialDelaySeconds: 15
      timeoutSeconds: 1
    replicas: 1
    resources: {}
    service:
      additionalPorts: []
      annotations: {}
      clusterIP: null
      labels: {}
      loadBalancerIP: null
      nodePort: null
      port: 80
      type: ClusterIP
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 6 }}
    verboseLogging: true
  global:
    clusterDomain: cluster.local
    dnsNamespace: kube-system
    dnsService: kube-dns
    image:
      pullSecrets: []
      registry: docker.io
    priorityClassName: null
  global_overrides:
    per_tenant_override_config: /runtime-config/overrides.yaml
  ingester:
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
              topologyKey: kubernetes.io/hostname
          - weight: 75
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    annotations: {}
    appProtocol:
      grpc: null
    autoscaling:
      behavior: {}
      enabled: false
      maxReplicas: 3
      minReplicas: 2
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    config:
      complete_block_timeout: null
      flush_all_on_shutdown: false
      flush_check_period: null
      max_block_bytes: null
      max_block_duration: null
      replication_factor: 1
      trace_idle_period: null
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    initContainers: []
    nodeSelector: {}
    persistence:
      annotations: {}
      enabled: false
      inMemory: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    service:
      annotations: {}
    terminationGracePeriodSeconds: 300
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 6 }}
    zoneAwareReplication:
      enabled: false
      maxUnavailable: 50
      topologyKey: null
      zones:
      - extraAffinity: {}
        name: zone-a
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-b
        nodeSelector: null
        storageClass: null
      - extraAffinity: {}
        name: zone-c
        nodeSelector: null
        storageClass: null
  license:
    contents: NOTAVALIDLICENSE
    external: false
    secretName: '{{ include "tempo.resourceName" (dict "ctx" . "component" "license")
      }}'
  memberlist:
    abort_if_cluster_join_fails: false
    bind_addr: []
    bind_port: 7946
    gossip_interval: 1s
    gossip_nodes: 2
    gossip_to_dead_nodes_time: 30s
    leave_timeout: 5s
    left_ingesters_timeout: 5m
    max_join_backoff: 1m
    max_join_retries: 10
    min_join_backoff: 1s
    node_name: ""
    packet_dial_timeout: 5s
    packet_write_timeout: 5s
    pull_push_interval: 30s
    randomize_node_name: true
    rejoin_interval: 0s
    retransmit_factor: 2
    stream_timeout: 10s
  memcached:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    enabled: true
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    host: memcached
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: null
      repository: memcached
      tag: 1.6.23-alpine
    podAnnotations: {}
    podLabels: {}
    replicas: 1
    resources: {}
    service:
      annotations: {}
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 6 }}
  memcachedExporter:
    enabled: false
    hostAliases: []
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: null
      repository: prom/memcached-exporter
      tag: v0.8.0
    resources: {}
  metaMonitoring:
    grafanaAgent:
      annotations: {}
      enabled: false
      installOperator: false
      labels: {}
      logs:
        additionalClientConfigs: []
        remote:
          auth:
            passwordSecretKey: ""
            passwordSecretName: ""
            tenantId: ""
            username: ""
          url: ""
      metrics:
        additionalRemoteWriteConfigs: []
        remote:
          auth:
            passwordSecretKey: ""
            passwordSecretName: ""
            username: ""
          headers: {}
          url: ""
        scrapeK8s:
          enabled: true
          kubeStateMetrics:
            labelSelectors:
              app.kubernetes.io/name: kube-state-metrics
            namespace: kube-system
      namespace: ""
    serviceMonitor:
      annotations: {}
      enabled: false
      interval: null
      labels: {}
      metricRelabelings: []
      namespace: null
      namespaceSelector: {}
      relabelings: []
      scheme: http
      scrapeTimeout: null
      tlsConfig: null
  metricsGenerator:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    annotations: {}
    appProtocol:
      grpc: null
    config:
      metrics_ingestion_time_range_slack: 30s
      processor:
        service_graphs:
          dimensions: []
          histogram_buckets:
          - 0.1
          - 0.2
          - 0.4
          - 0.8
          - 1.6
          - 3.2
          - 6.4
          - 12.8
          max_items: 10000
          wait: 10s
          workers: 10
        span_metrics:
          dimensions: []
          histogram_buckets:
          - 0.002
          - 0.004
          - 0.008
          - 0.016
          - 0.032
          - 0.064
          - 0.128
          - 0.256
          - 0.512
          - 1.02
          - 2.05
          - 4.1
      registry:
        collection_interval: 15s
        external_labels: {}
        stale_duration: 15m
      storage:
        path: /var/tempo/wal
        remote_write: []
        remote_write_add_org_id_header: true
        remote_write_flush_deadline: 1m
        wal: null
      traces_storage:
        path: /var/tempo/traces
    enabled: false
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    initContainers: []
    kind: Deployment
    nodeSelector: {}
    persistence:
      annotations: {}
      enabled: false
      size: 10Gi
      storageClass: null
    podAnnotations: {}
    podLabels: {}
    ports:
    - name: grpc
      port: 9095
      service: true
    - name: http-memberlist
      port: 7946
      service: false
    - name: http-metrics
      port: 3100
      service: true
    priorityClassName: null
    replicas: 1
    resources: {}
    service:
      annotations: {}
    terminationGracePeriodSeconds: 300
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 6 }}
    walEmptyDir: {}
  minio:
    buckets:
    - name: tempo-traces
      policy: none
      purge: false
    - name: enterprise-traces
      policy: none
      purge: false
    - name: enterprise-traces-admin
      policy: none
      purge: false
    configPathmc: /tmp/minio/mc/
    enabled: false
    mode: standalone
    persistence:
      size: 5Gi
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
    rootPassword: supersecret
    rootUser: grafana-tempo
  multitenancyEnabled: false
  overrides: |
    overrides: {}
  prometheusRule:
    annotations: {}
    enabled: false
    groups: []
    labels: {}
    namespace: null
  querier:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    appProtocol:
      grpc: null
    autoscaling:
      behavior: {}
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    config:
      frontend_worker:
        grpc_client_config: {}
      max_concurrent_queries: 20
      search:
        external_backend: ""
        external_endpoints: []
        external_hedge_requests_at: 8s
        external_hedge_requests_up_to: 2
        google_cloud_run: {}
        prefer_self: 10
        query_timeout: 30s
      trace_by_id:
        query_timeout: 10s
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    replicas: 1
    resources: {}
    service:
      annotations: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
       q whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier") | nindent 6 }}
  queryFrontend:
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    appProtocol:
      grpc: null
    autoscaling:
      behavior: {}
      enabled: false
      maxReplicas: 3
      minReplicas: 1
      targetCPUUtilizationPercentage: 60
      targetMemoryUtilizationPercentage: null
    config:
      max_outstanding_per_tenant: 2000
      max_retries: 2
      search:
        concurrent_jobs: 1000
        target_bytes_per_job: 104857600
      trace_by_id:
        query_shards: 50
    extraArgs: []
    extraEnv: []
    extraEnvFrom: []
    extraVolumeMounts: []
    extraVolumes: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    ingress:
      annotations: {}
      enabled: false
      hosts:
      - host: query.tempo.example.com
        paths:
        - path: /
      tls:
      - hosts:
        - query.tempo.example.com
        secretName: tempo-query-tls
    nodeSelector: {}
    podAnnotations: {}
    podLabels: {}
    priorityClassName: null
    query:
      config: |
        backend: 127.0.0.1:3100
      enabled: false
      extraArgs: []
      extraEnv: []
      extraEnvFrom: []
      extraVolumeMounts: []
      extraVolumes: []
      image:
        pullSecrets: []
        registry: null
        repository: grafana/tempo-query
        tag: null
      resources: {}
    replicas: 1
    resources: {}
    service:
      annotations: {}
      labels: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      port: 16686
      type: ClusterIP
    serviceDiscovery:
      annotations: {}
      labels: {}
    terminationGracePeriodSeconds: 30
    tolerations: []
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 6 }}
  rbac:
    create: false
    pspEnabled: false
  reportingEnabled: true
  rollout_operator:
    enabled: false
    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
      seccompProfile:
        type: RuntimeDefault
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
  server:
    grpc_server_max_recv_msg_size: 4194304
    grpc_server_max_send_msg_size: 4194304
    http_server_read_timeout: 30s
    http_server_write_timeout: 30s
    httpListenPort: 3100
    logFormat: logfmt
    logLevel: info
  serviceAccount:
    annotations: {}
    automountServiceAccountToken: false
    create: true
    imagePullSecrets: []
    name: null
  storage:
    admin:
      backend: filesystem
    trace:
      backend: local
      block:
        dedicated_columns: []
        version: null
      pool:
        max_workers: 400
        queue_depth: 20000
  tempo:
    image:
      pullPolicy: IfNotPresent
      pullSecrets: []
      registry: docker.io
      repository: grafana/tempo
      tag: null
    memberlist:
      appProtocol: null
    podAnnotations: {}
    podLabels: {}
    podSecurityContext:
      fsGroup: 1000
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 30
      timeoutSeconds: 1
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    structuredConfig: {}
  tokengenJob:
    annotations: {}
    containerSecurityContext:
      readOnlyRootFilesystem: true
    enable: true
    env: []
    extraArgs: {}
    extraEnvFrom: []
    hostAliases: []
    image:
      pullSecrets: []
      registry: null
      repository: null
      tag: null
    initContainers: []
  traces:
    jaeger:
      grpc:
        enabled: false
        receiverConfig: {}
      thriftBinary:
        enabled: false
        receiverConfig: {}
      thriftCompact:
        enabled: false
        receiverConfig: {}
      thriftHttp:
        enabled: false
        receiverConfig: {}
    kafka: {}
    opencensus:
      enabled: false
      receiverConfig: {}
    otlp:
      grpc:
        enabled: true
        receiverConfig: {
#          endpoint: localhost:4317
        }
      http:
        enabled: false
        receiverConfig: {
          endpoint: localhost:4318
        }
    zipkin:
      enabled: false
      receiverConfig: {}
  useExternalConfig: false
